{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D,Conv2DTranspose\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from random import randint\n",
    "import tensorflow as tf\n",
    "from utilities import *\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def custom_loss(y_true, y_pred):\n",
    "        return 100 * tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: python main.py [train, sample]\n"
     ]
    }
   ],
   "source": [
    "class CCGAN():\n",
    "    def __init__(self, batch_size = 4):\n",
    "        # input image being 256 * 256 * 3&1\n",
    "        self.img_rows = 256\n",
    "        self.img_cols = 256\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_sqrt = int(math.sqrt(self.batch_size))\n",
    "        \n",
    "        #color hint RGB has 3\n",
    "        self.color_channels = 3\n",
    "        \n",
    "        #line image has 1\n",
    "        self.channels = 1\n",
    "        \n",
    "        self.color_img_shape = (self.img_rows, self.img_cols, self.color_channels)\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        # Use standrad AdamOptimizer with 2e-4 and 0.5 beta\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        \n",
    "        \"\"\"\n",
    "        the general keras GAN network for conv2d uses a mask to randomly slice an image of height * width from the training images; don't know if I should use in the color one\"\n",
    "\n",
    "        \"\"\"\n",
    "        self.mask_height = 100\n",
    "        self.mask_width = 100\n",
    "        \n",
    "        #self.num_classes = 1\n",
    "\n",
    "        # Number of filters in first layer of generator and discriminator\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes the color hint and the line image as input\n",
    "        Color_hint = Input(shape=self.color_img_shape)\n",
    "        Line_image = Input(shape=self.img_shape)\n",
    "        \n",
    "        gen_img = self.generator([Line_image, Color_hint])\n",
    "\n",
    "        # See if train discriminator or not\n",
    "        #self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity - the chance of it being true images that discriminator thinks\n",
    "        validity = self.discriminator(gen_img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "\n",
    "        self.combined = Model(inputs=[Color_hint, Line_image], outputs = [validity, gen_img])\n",
    "        self.combined.compile(loss=['binary_crossentropy', custom_loss],\n",
    "            loss_weights=[0.5, 0.5],\n",
    "            optimizer=optimizer )\n",
    "        \n",
    "        self.generator.summary()\n",
    "        self.combined.summary()\n",
    "        \n",
    "        \n",
    "    def build_generator(self):\n",
    "        \"\"\"input:None\n",
    "            output: a model object(special keras object) with attributes of defined inputs and outputs\n",
    "            \n",
    "            use this function to create generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=5, bn=True):\n",
    "            \"\"\"Layers used during downsampling\n",
    "            input: (input, # of filters, default size of filter being 5x5, batchnormalization default true)\n",
    "            f_size could be 4; to be tested to find optimum\n",
    "            \"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.9)(d)\n",
    "                #hyper-parameter here. Tpyical conv2d GAN nets I checked used a 0.8 momentum to normalize \n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=5, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            #u = UpSampling2D(size=2)(layer_input)\n",
    "            #u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            u =  Conv2DTranspose(filters, kernel_size=f_size, strides=2, padding='same', activation='relu')(layer_input)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.9)(u)\n",
    "            #a residual layer that concats this and previous layer\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            #u = tf.concat([u,skip_input], axis = 3)\n",
    "            return u\n",
    "\n",
    "        color_img = Input(shape=self.color_img_shape)\n",
    "        Line_img = Input(shape=self.img_shape)\n",
    "        img = Concatenate()([Line_img, color_img])\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(img, self.gf, bn=False)\n",
    "        #d1 is (128 x 128 x self.gf_dim) [height x width x #of channels]\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        #d2 is (64 x 64 x self.gf_dim*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        #d3 is (32 x 32 x self.gf_dim*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        #d4 is (16 x 16 x self.gf_dim*8)\n",
    "        d5 = conv2d(d4, self.gf*8)\n",
    "        #d5 is (8 x 8 x self.gf_dim*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d5, d4, self.gf*8)\n",
    "        #u1 is (16 x 16 x self.gf_dim*8 * 2) p.s. * 2 is because we concat deconv2d(d5) and d4, both of shape (16 x 16 x self.gf_dim*8)\n",
    "        u2 = deconv2d(u1, d3, self.gf*4)\n",
    "        #u2 is (32 x 32 x self.gf_dim*4 * 2)\n",
    "        u3 = deconv2d(u2, d2, self.gf*2)\n",
    "        #u3 is (64 x 64 x self.gf_dim*2 * 2)\n",
    "        u4 = deconv2d(u3, d1, self.gf)\n",
    "        #u4 is (128 x 128 x self.gf_dim * 2)\n",
    "        u5 = UpSampling2D(size=2)(u4)\n",
    "\n",
    "        #u5 is unsampled from u4 back to original image type (256 x 256 x 3)\n",
    "        \n",
    "        #outout_img = u5\n",
    "        #here typical conv2d adds another convolutional layer to transofrm; I don't know if this is necessary as the source code in deep color didn't did so\n",
    "        #also note that the convolutional size formula is (W−F+2P)/S+1; here W being 256, so F must be odd so that given Strides being 1 there exsits some P in which (W−F+2P)/S+1 = W and spatial dimensions are preserved\n",
    "        output_img = Conv2D(self.color_channels, kernel_size=5, strides=1, padding='same', activation='tanh')(u5)\n",
    "        return Model(inputs = [color_img, Line_img], outputs = output_img)\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        img = Input(shape=self.color_img_shape)\n",
    "        \n",
    "        #I make batchnormalization being implemented after relu activation, a commmon sense in deep learning I believe which outperforms the opposite way, even though author in deep color uses batchnormalization before relu activation\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(self.df, kernel_size=5, strides=2, padding='same', input_shape=self.color_img_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        # here output is (128 * 128 * self.df)\n",
    "        model.add(Conv2D(self.df * 2, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(InstanceNormalization())\n",
    "        # here output is (64 * 64 * self.df * 2)\n",
    "        model.add(Conv2D(self.df * 4, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(InstanceNormalization())\n",
    "        # here output is (32 * 32 * self.df * 4)\n",
    "        model.add(Conv2D(self.df * 8, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(InstanceNormalization())\n",
    "        # here output is (16 * 16 * self.df * 8)\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def imageblur(self, cimg, sampling=False):\n",
    "        if sampling:\n",
    "            cimg = cimg * 0.3 + np.ones_like(cimg) * 0.7 * 255\n",
    "        else:\n",
    "            for i in range(30):\n",
    "                randx = randint(0,205)\n",
    "                randy = randint(0,205)\n",
    "                cimg[randx:randx+50, randy:randy+50] = 255\n",
    "        return cv2.blur(cimg,(100,100))\n",
    "    \n",
    "\n",
    "    def train(self, epochs = 20000, sample_interval = 100):\n",
    "        \n",
    "        #load the weights to model\n",
    "        self.load_model()\n",
    "\n",
    "        # Load the dataset\n",
    "        data = glob(os.path.join('imgs', '*.jpg'))\n",
    "        print(data[0])\n",
    "\n",
    "        #base image to show\n",
    "        base = np.array([get_image(sample_file) for sample_file in data[0:self.batch_size]])\n",
    "        base_normalized = base/255.0\n",
    "\n",
    "        #line image to show\n",
    "        base_edge = np.array([cv2.adaptiveThreshold(cv2.cvtColor(ba, cv2.COLOR_BGR2GRAY), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=9, C=2) for ba in base]) / 255.0\n",
    "        base_edge = np.expand_dims(base_edge, 3)\n",
    "\n",
    "        #colorhint\n",
    "        base_colors = np.array([self.imageblur(ba) for ba in base]) / 255.0\n",
    "\n",
    "        ims(\"results/base.png\",merge_color(base_normalized, [self.batch_size_sqrt, self.batch_size_sqrt]))\n",
    "        ims(\"results/base_line.jpg\",merge(base_edge, [self.batch_size_sqrt, self.batch_size_sqrt]))\n",
    "        ims(\"results/base_colors.jpg\",merge_color(base_colors, [self.batch_size_sqrt, self.batch_size_sqrt]))        \n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((self.batch_size, 1))\n",
    "        fake = np.zeros((self.batch_size, 1))\n",
    "\n",
    "        datalen = len(data)\n",
    "\n",
    "        for e in range(epochs):\n",
    "            for i in range(datalen//self.batch_size):\n",
    "\n",
    "                #get one batch of data\n",
    "                batch_files = data[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                batch = np.array([get_image(batch_file) for batch_file in batch_files])\n",
    "                batch_normalized = batch/255.0\n",
    "\n",
    "                batch_edge = np.array([cv2.adaptiveThreshold(cv2.cvtColor(ba, cv2.COLOR_BGR2GRAY), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=9, C=2) for ba in batch]) / 255.0\n",
    "                batch_edge = np.expand_dims(batch_edge, 3)\n",
    "\n",
    "                batch_colors = np.array([self.imageblur(ba) for ba in batch]) / 255.0\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict([batch_colors, batch_edge])\n",
    "\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch(batch_normalized, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Train the generator\n",
    "                g_loss = self.combined.train_on_batch([batch_colors, batch_edge], [valid, batch_normalized])\n",
    "\n",
    "                # Plot the progress\n",
    "                #print(\"%d: [%d / %d] d_loss1 %f, d_loss2 %f, g_loss %f\" % (e, i, (datalen//self.batch_size), d_loss[0],d_loss[1], g_loss))\n",
    "                print(e, i, (datalen//self.batch_size), d_loss, g_loss)\n",
    "                # If at save interval => save generated image samples\n",
    "                if i % sample_interval == 0:\n",
    "                    # Select a random half batch of       \n",
    "                    self.sample()\n",
    "                \n",
    "                if i % (5*sample_interval) == 0:\n",
    "                    self.save_model()\n",
    "                    \n",
    "    def sample(self):\n",
    "        self.load_model(False)\n",
    "\n",
    "        data = glob(os.path.join(\"imgs\", \"*.jpg\"))\n",
    "\n",
    "        datalen = len(data)\n",
    "\n",
    "        for i in range(5):\n",
    "            batch_files = data[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            batch = np.array([cv2.resize(imread(batch_file), (256,256)) for batch_file in batch_files])\n",
    "            batch_normalized = batch/255.0\n",
    "\n",
    "            batch_edge = np.array([cv2.adaptiveThreshold(cv2.cvtColor(ba, cv2.COLOR_BGR2GRAY), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=9, C=2) for ba in batch]) / 255.0\n",
    "            batch_edge = np.expand_dims(batch_edge, 3)\n",
    "\n",
    "            batch_colors = np.array([self.imageblur(ba,True) for ba in batch]) / 255.0\n",
    "\n",
    "            recreation = self.generator.predict([batch_colors, batch_edge])\n",
    "            ims(\"results/sample_\"+str(i)+\".jpg\",merge_color(recreation, [self.batch_size_sqrt, self.batch_size_sqrt]))\n",
    "            ims(\"results/sample_\"+str(i)+\"_origin.jpg\",merge_color(batch_normalized, [self.batch_size_sqrt, self.batch_size_sqrt]))\n",
    "            ims(\"results/sample_\"+str(i)+\"_line.jpg\",merge_color(batch_edge, [self.batch_size_sqrt, self.batch_size_sqrt]))\n",
    "            ims(\"results/sample_\"+str(i)+\"_color.jpg\",merge_color(batch_colors, [self.batch_size_sqrt, self.batch_size_sqrt]))\n",
    "                    \n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        self.generator.save('ccgan_generator.h5')\n",
    "        self.discriminator.save('ccgan_generator.h5')\n",
    "        self.combined.save('ccgan.h5')\n",
    "    \n",
    "    def load_model(self, load_disc = True, first = True):\n",
    "        if first == False:\n",
    "            self.generator.load('ccgan_generator.h5') \n",
    "            self.generator.summary()\n",
    "\n",
    "            if load_disc:\n",
    "                self.discriminator.load('ccgan_generator.h5')\n",
    "                self.combined.load('ccgan.h5') \n",
    "                self.discriminator.summary()\n",
    "                self.combined.summary()\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def delete_model(disc = True, gen = True):\n",
    "        if disc:\n",
    "            del self.discriminator\n",
    "        if gen:\n",
    "            del self.generator\n",
    "\n",
    "\n",
    "        '''\n",
    "        def save(model, model_name):\n",
    "            model_path = \"saved_model/%s.json\" % model_name\n",
    "            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "        save(self.generator, \"ccgan_generator\")\n",
    "        save(self.discriminator, \"ccgan_discriminator\")\n",
    "        '''\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python main.py [train, sample]\")\n",
    "    else:\n",
    "        cmd = sys.argv[1]\n",
    "        if cmd == \"train\":\n",
    "            c = Color()\n",
    "            c.train()\n",
    "        elif cmd == \"sample\":\n",
    "            c = Color(512,1)\n",
    "            c.sample()\n",
    "        else:\n",
    "            print(\"Usage: python main.py [train, sample]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_51 (Conv2D)           (None, 128, 128, 64)      4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 64, 64, 128)       204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_16 (I (None, 64, 64, 128)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 32, 32, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_17 (I (None, 32, 32, 256)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 16, 16, 512)       3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_18 (I (None, 16, 16, 512)       2         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 131072)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 131073    \n",
      "=================================================================\n",
      "Total params: 4,437,639\n",
      "Trainable params: 4,437,639\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 256, 256, 4)  0           input_29[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 128, 128, 64) 6464        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)      (None, 128, 128, 64) 0           conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 64, 64, 128)  204928      leaky_re_lu_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)      (None, 64, 64, 128)  0           conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 64, 64, 128)  512         leaky_re_lu_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 32, 32, 256)  819456      batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)      (None, 32, 32, 256)  0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 32, 32, 256)  1024        leaky_re_lu_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 16, 512)  3277312     batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)      (None, 16, 16, 512)  0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 16, 16, 512)  2048        leaky_re_lu_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 8, 8, 512)    6554112     batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)      (None, 8, 8, 512)    0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 8, 8, 512)    2048        leaky_re_lu_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_21 (Conv2DTran (None, 16, 16, 512)  6554112     batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 16, 16, 512)  2048        conv2d_transpose_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 16, 16, 1024) 0           batch_normalization_45[0][0]     \n",
      "                                                                 batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_22 (Conv2DTran (None, 32, 32, 256)  6553856     concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 32, 32, 256)  1024        conv2d_transpose_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 32, 32, 512)  0           batch_normalization_46[0][0]     \n",
      "                                                                 batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_23 (Conv2DTran (None, 64, 64, 128)  1638528     concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 64, 64, 128)  512         conv2d_transpose_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 64, 64, 256)  0           batch_normalization_47[0][0]     \n",
      "                                                                 batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_24 (Conv2DTran (None, 128, 128, 64) 409664      concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 128, 128, 64) 256         conv2d_transpose_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 128, 128, 128 0           batch_normalization_48[0][0]     \n",
      "                                                                 leaky_re_lu_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 256, 256, 128 0           concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 256, 256, 3)  9603        up_sampling2d_6[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 26,037,507\n",
      "Trainable params: 26,032,771\n",
      "Non-trainable params: 4,736\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_17 (Model)                (None, 256, 256, 3)  26037507    input_31[0][0]                   \n",
      "                                                                 input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_16 (Model)                (None, 1)            4437639     model_17[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 30,475,146\n",
      "Trainable params: 30,470,410\n",
      "Non-trainable params: 4,736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "colorblind = CCGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_color(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "\n",
    "    return img\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 1))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving img results/sample_0.jpg\n",
      "saving img results/sample_0_origin.jpg\n",
      "saving img results/sample_0_line.jpg\n",
      "saving img results/sample_0_color.jpg\n",
      "saving img results/sample_1.jpg\n",
      "saving img results/sample_1_origin.jpg\n",
      "saving img results/sample_1_line.jpg\n",
      "saving img results/sample_1_color.jpg\n",
      "saving img results/sample_2.jpg\n",
      "saving img results/sample_2_origin.jpg\n",
      "saving img results/sample_2_line.jpg\n",
      "saving img results/sample_2_color.jpg\n",
      "saving img results/sample_3.jpg\n",
      "saving img results/sample_3_origin.jpg\n",
      "saving img results/sample_3_line.jpg\n",
      "saving img results/sample_3_color.jpg\n",
      "saving img results/sample_4.jpg\n",
      "saving img results/sample_4_origin.jpg\n",
      "saving img results/sample_4_line.jpg\n",
      "saving img results/sample_4_color.jpg\n",
      "saving img results/sample_5.jpg\n",
      "saving img results/sample_5_origin.jpg\n",
      "saving img results/sample_5_line.jpg\n",
      "saving img results/sample_5_color.jpg\n",
      "saving img results/sample_6.jpg\n",
      "saving img results/sample_6_origin.jpg\n",
      "saving img results/sample_6_line.jpg\n",
      "saving img results/sample_6_color.jpg\n",
      "saving img results/sample_7.jpg\n",
      "saving img results/sample_7_origin.jpg\n",
      "saving img results/sample_7_line.jpg\n",
      "saving img results/sample_7_color.jpg\n",
      "saving img results/sample_8.jpg\n",
      "saving img results/sample_8_origin.jpg\n",
      "saving img results/sample_8_line.jpg\n",
      "saving img results/sample_8_color.jpg\n",
      "saving img results/sample_9.jpg\n",
      "saving img results/sample_9_origin.jpg\n",
      "saving img results/sample_9_line.jpg\n",
      "saving img results/sample_9_color.jpg\n",
      "saving img results/sample_10.jpg\n",
      "saving img results/sample_10_origin.jpg\n",
      "saving img results/sample_10_line.jpg\n",
      "saving img results/sample_10_color.jpg\n",
      "saving img results/sample_11.jpg\n",
      "saving img results/sample_11_origin.jpg\n",
      "saving img results/sample_11_line.jpg\n",
      "saving img results/sample_11_color.jpg\n",
      "saving img results/sample_12.jpg\n",
      "saving img results/sample_12_origin.jpg\n",
      "saving img results/sample_12_line.jpg\n",
      "saving img results/sample_12_color.jpg\n",
      "saving img results/sample_13.jpg\n",
      "saving img results/sample_13_origin.jpg\n",
      "saving img results/sample_13_line.jpg\n",
      "saving img results/sample_13_color.jpg\n",
      "saving img results/sample_14.jpg\n",
      "saving img results/sample_14_origin.jpg\n",
      "saving img results/sample_14_line.jpg\n",
      "saving img results/sample_14_color.jpg\n",
      "saving img results/sample_15.jpg\n",
      "saving img results/sample_15_origin.jpg\n",
      "saving img results/sample_15_line.jpg\n",
      "saving img results/sample_15_color.jpg\n",
      "saving img results/sample_16.jpg\n",
      "saving img results/sample_16_origin.jpg\n",
      "saving img results/sample_16_line.jpg\n",
      "saving img results/sample_16_color.jpg\n",
      "saving img results/sample_17.jpg\n",
      "saving img results/sample_17_origin.jpg\n",
      "saving img results/sample_17_line.jpg\n",
      "saving img results/sample_17_color.jpg\n",
      "saving img results/sample_18.jpg\n",
      "saving img results/sample_18_origin.jpg\n",
      "saving img results/sample_18_line.jpg\n",
      "saving img results/sample_18_color.jpg\n",
      "saving img results/sample_19.jpg\n",
      "saving img results/sample_19_origin.jpg\n",
      "saving img results/sample_19_line.jpg\n",
      "saving img results/sample_19_color.jpg\n",
      "saving img results/sample_20.jpg\n",
      "saving img results/sample_20_origin.jpg\n",
      "saving img results/sample_20_line.jpg\n",
      "saving img results/sample_20_color.jpg\n",
      "saving img results/sample_21.jpg\n",
      "saving img results/sample_21_origin.jpg\n",
      "saving img results/sample_21_line.jpg\n",
      "saving img results/sample_21_color.jpg\n",
      "saving img results/sample_22.jpg\n",
      "saving img results/sample_22_origin.jpg\n",
      "saving img results/sample_22_line.jpg\n",
      "saving img results/sample_22_color.jpg\n",
      "saving img results/sample_23.jpg\n",
      "saving img results/sample_23_origin.jpg\n",
      "saving img results/sample_23_line.jpg\n",
      "saving img results/sample_23_color.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-358e0056c633>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcolorblind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-041913252e1f>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[0mbatch_colors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimageblur\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mba\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mba\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m             \u001b[0mrecreation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_colors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_edge\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m             \u001b[0mims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"results/sample_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".jpg\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmerge_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecreation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size_sqrt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size_sqrt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[0mims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"results/sample_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_origin.jpg\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmerge_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_normalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size_sqrt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size_sqrt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "colorblind.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs\\00006fde3d3b96a3d8e5f37236e5a2e4.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/base.png\n",
      "saving img results/base_line.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/base_colors.jpg\n",
      "0 0 14257 [2.987928 0.      ] [50.637783, 16.118095, 85.15747]\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_0.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_0_origin.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_0_line.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_0_color.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_1.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_1_origin.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_1_line.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_1_color.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_2.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_2_origin.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_2_line.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_2_color.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_3.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_3_origin.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_3_line.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_3_color.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_4.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_4_origin.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_4_line.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_4_color.jpg\n",
      "0 1 14257 [8.059048 0.      ] [42.661556, 16.118095, 69.20502]\n",
      "0 2 14257 [8.059048 0.      ] [34.137383, 16.118095, 52.15667]\n",
      "0 3 14257 [8.059048 0.      ] [30.860916, 16.118095, 45.603737]\n",
      "0 4 14257 [8.059048 0.      ] [26.641668, 16.118095, 37.16524]\n",
      "0 5 14257 [8.059048 0.      ] [26.439243, 16.118095, 36.76039]\n",
      "0 6 14257 [8.059048 0.      ] [20.567127, 16.118095, 25.016161]\n",
      "0 7 14257 [8.059048 0.      ] [24.660124, 16.118095, 33.202152]\n",
      "0 8 14257 [8.059048 0.      ] [18.940058, 16.118095, 21.76202]\n",
      "0 9 14257 [8.059048 0.      ] [20.42388, 16.118095, 24.729664]\n",
      "0 10 14257 [8.059048 0.      ] [19.464077, 16.118095, 22.81006]\n",
      "0 11 14257 [8.059048 0.      ] [19.543491, 16.118095, 22.968887]\n",
      "0 12 14257 [8.059048 0.      ] [24.002384, 16.118095, 31.886673]\n",
      "0 13 14257 [8.059048 0.      ] [19.797447, 16.118095, 23.476799]\n",
      "0 14 14257 [8.059048 0.      ] [16.79842, 16.118095, 17.478743]\n",
      "0 15 14257 [8.059048 0.      ] [16.879566, 16.118095, 17.641039]\n",
      "0 16 14257 [8.059048 0.      ] [15.241085, 16.118095, 14.364074]\n",
      "0 17 14257 [8.059048 0.      ] [20.4332, 16.118095, 24.748308]\n",
      "0 18 14257 [8.059048 0.      ] [14.441234, 16.118095, 12.764372]\n",
      "0 19 14257 [8.059048 0.      ] [18.999722, 16.118095, 21.88135]\n",
      "0 20 14257 [8.059048 0.      ] [22.24964, 16.118095, 28.381184]\n",
      "0 21 14257 [8.059048 0.      ] [18.630802, 16.118095, 21.143509]\n",
      "0 22 14257 [8.059048 0.      ] [14.344229, 16.118095, 12.570362]\n",
      "0 23 14257 [8.059048 0.      ] [17.55473, 16.118095, 18.991364]\n",
      "0 24 14257 [8.059048 0.      ] [16.97876, 16.118095, 17.839422]\n",
      "0 25 14257 [8.059048 0.      ] [18.124851, 16.118095, 20.131605]\n",
      "0 26 14257 [8.059048 0.      ] [16.104574, 16.118095, 16.091053]\n",
      "0 27 14257 [8.059048 0.      ] [15.443402, 16.118095, 14.768709]\n",
      "0 28 14257 [8.059048 0.      ] [14.719988, 16.118095, 13.321881]\n",
      "0 29 14257 [8.059048 0.      ] [13.850061, 16.118095, 11.5820265]\n",
      "0 30 14257 [8.059048 0.      ] [17.789364, 16.118095, 19.460632]\n",
      "0 31 14257 [8.059048 0.      ] [14.655149, 16.118095, 13.192203]\n",
      "0 32 14257 [8.059048 0.      ] [21.497353, 16.118095, 26.876608]\n",
      "0 33 14257 [8.059048 0.      ] [15.344817, 16.118095, 14.571539]\n",
      "0 34 14257 [8.059048 0.      ] [20.318958, 16.118095, 24.519823]\n",
      "0 35 14257 [8.059048 0.      ] [17.285503, 16.118095, 18.452911]\n",
      "0 36 14257 [8.059048 0.      ] [15.387429, 16.118095, 14.656763]\n",
      "0 37 14257 [8.059048 0.      ] [20.099098, 16.118095, 24.0801]\n",
      "0 38 14257 [8.059048 0.      ] [15.539385, 16.118095, 14.960675]\n",
      "0 39 14257 [8.059048 0.      ] [13.982897, 16.118095, 11.847698]\n",
      "0 40 14257 [8.059048 0.      ] [16.534592, 16.118095, 16.951086]\n",
      "0 41 14257 [8.059048 0.      ] [14.624373, 16.118095, 13.130651]\n",
      "0 42 14257 [8.059048 0.      ] [14.164936, 16.118095, 12.211776]\n",
      "0 43 14257 [8.059048 0.      ] [18.647652, 16.118095, 21.177208]\n",
      "0 44 14257 [8.059048 0.      ] [16.194386, 16.118095, 16.270674]\n",
      "0 45 14257 [8.059048 0.      ] [18.430824, 16.118095, 20.743553]\n",
      "0 46 14257 [8.059048 0.      ] [17.26668, 16.118095, 18.415264]\n",
      "0 47 14257 [8.059048 0.      ] [13.000092, 16.118095, 9.882089]\n",
      "0 48 14257 [8.059048 0.      ] [15.80731, 16.118095, 15.496524]\n",
      "0 49 14257 [8.059048 0.      ] [16.001417, 16.118095, 15.884738]\n",
      "0 50 14257 [8.059048 0.      ] [16.280767, 16.118095, 16.44344]\n",
      "0 51 14257 [8.059048 0.      ] [16.67855, 16.118095, 17.239006]\n",
      "0 52 14257 [8.059048 0.      ] [17.774849, 16.118095, 19.431602]\n",
      "0 53 14257 [8.059048 0.      ] [14.275972, 16.118095, 12.433848]\n",
      "0 54 14257 [8.059048 0.      ] [13.120194, 16.118095, 10.122293]\n",
      "0 55 14257 [8.059048 0.      ] [19.511538, 16.118095, 22.90498]\n",
      "0 56 14257 [8.059048 0.      ] [15.069119, 16.118095, 14.0201435]\n",
      "0 57 14257 [8.059048 0.      ] [15.432072, 16.118095, 14.746048]\n",
      "0 58 14257 [8.059048 0.      ] [16.90316, 16.118095, 17.688227]\n",
      "0 59 14257 [8.059048 0.      ] [15.245192, 16.118095, 14.372288]\n",
      "0 60 14257 [8.059048 0.      ] [16.686619, 16.118095, 17.255144]\n",
      "0 61 14257 [8.059048 0.      ] [17.853405, 16.118095, 19.588715]\n",
      "0 62 14257 [8.059048 0.      ] [17.845634, 16.118095, 19.573175]\n",
      "0 63 14257 [8.059048 0.      ] [15.521188, 16.118095, 14.92428]\n",
      "0 64 14257 [8.059048 0.      ] [21.034876, 16.118095, 25.951656]\n",
      "0 65 14257 [8.059048 0.      ] [15.034813, 16.118095, 13.9515295]\n",
      "0 66 14257 [8.059048 0.      ] [17.148375, 16.118095, 18.178654]\n",
      "0 67 14257 [8.059048 0.      ] [17.992327, 16.118095, 19.866558]\n",
      "0 68 14257 [8.059048 0.      ] [18.561016, 16.118095, 21.003937]\n",
      "0 69 14257 [8.059048 0.      ] [14.159233, 16.118095, 12.200371]\n",
      "0 70 14257 [8.059048 0.      ] [24.669111, 16.118095, 33.220127]\n",
      "0 71 14257 [8.059048 0.      ] [18.176203, 16.118095, 20.23431]\n",
      "0 72 14257 [8.059048 0.      ] [16.81958, 16.118095, 17.521065]\n",
      "0 73 14257 [8.059048 0.      ] [17.364979, 16.118095, 18.61186]\n",
      "0 74 14257 [8.059048 0.      ] [13.485928, 16.118095, 10.853759]\n",
      "0 75 14257 [8.059048 0.      ] [18.05745, 16.118095, 19.996803]\n",
      "0 76 14257 [8.059048 0.      ] [18.872091, 16.118095, 21.626087]\n",
      "0 77 14257 [8.059048 0.      ] [15.883463, 16.118095, 15.64883]\n",
      "0 78 14257 [8.059048 0.      ] [19.555056, 16.118095, 22.992016]\n",
      "0 79 14257 [8.059048 0.      ] [16.976318, 16.118095, 17.83454]\n",
      "0 80 14257 [8.059048 0.      ] [18.305662, 16.118095, 20.493229]\n",
      "0 81 14257 [8.059048 0.      ] [18.025103, 16.118095, 19.93211]\n",
      "0 82 14257 [8.059048 0.      ] [16.163765, 16.118095, 16.209435]\n",
      "0 83 14257 [8.059048 0.      ] [15.984128, 16.118095, 15.850161]\n",
      "0 84 14257 [8.059048 0.      ] [15.8414, 16.118095, 15.564704]\n",
      "0 85 14257 [8.059048 0.      ] [16.930279, 16.118095, 17.742462]\n",
      "0 86 14257 [8.059048 0.      ] [16.955938, 16.118095, 17.793781]\n",
      "0 87 14257 [8.059048 0.      ] [14.811921, 16.118095, 13.505747]\n",
      "0 88 14257 [8.059048 0.      ] [18.76423, 16.118095, 21.410366]\n",
      "0 89 14257 [8.059048 0.      ] [17.074158, 16.118095, 18.030218]\n",
      "0 90 14257 [8.059048 0.      ] [17.74062, 16.118095, 19.363146]\n",
      "0 91 14257 [8.059048 0.      ] [16.517262, 16.118095, 16.916426]\n",
      "0 92 14257 [8.059048 0.      ] [15.638718, 16.118095, 15.15934]\n",
      "0 93 14257 [8.059048 0.      ] [16.880756, 16.118095, 17.643415]\n",
      "0 94 14257 [8.059048 0.      ] [20.670036, 16.118095, 25.22198]\n",
      "0 95 14257 [8.059048 0.      ] [21.146168, 16.118095, 26.174238]\n",
      "0 96 14257 [8.059048 0.      ] [17.174751, 16.118095, 18.23141]\n",
      "0 97 14257 [8.059048 0.      ] [16.113684, 16.118095, 16.109272]\n",
      "0 98 14257 [8.059048 0.      ] [14.106525, 16.118095, 12.0949545]\n",
      "0 99 14257 [8.059048 0.      ] [16.885538, 16.118095, 17.65298]\n",
      "0 100 14257 [8.059048 0.      ] [16.90818, 16.118095, 17.698265]\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_0.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_0_origin.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_0_line.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_0_color.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_1.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_1_origin.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_1_line.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_1_color.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_2.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_2_origin.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_2_line.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_2_color.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_3.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_3_origin.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_3_line.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_3_color.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_4.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_4_origin.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_4_line.jpg\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "saving img results/sample_4_color.jpg\n",
      "0 101 14257 [8.059048 0.      ] [15.505499, 16.118095, 14.892901]\n",
      "0 102 14257 [8.059048 0.      ] [19.949343, 16.118095, 23.78059]\n",
      "0 103 14257 [8.059048 0.      ] [15.874394, 16.118095, 15.630693]\n",
      "0 104 14257 [8.059048 0.      ] [18.273254, 16.118095, 20.428411]\n",
      "0 105 14257 [8.059048 0.      ] [13.616841, 16.118095, 11.115587]\n",
      "0 106 14257 [8.059048 0.      ] [15.89851, 16.118095, 15.6789255]\n",
      "0 107 14257 [8.059048 0.      ] [16.746164, 16.118095, 17.374233]\n",
      "0 108 14257 [8.059048 0.      ] [15.78717, 16.118095, 15.456244]\n",
      "0 109 14257 [8.059048 0.      ] [15.493837, 16.118095, 14.869578]\n",
      "0 110 14257 [8.059048 0.      ] [18.018204, 16.118095, 19.918312]\n",
      "0 111 14257 [8.059048 0.      ] [20.7631, 16.118095, 25.408104]\n",
      "0 112 14257 [8.059048 0.      ] [16.530464, 16.118095, 16.942833]\n",
      "0 113 14257 [8.059048 0.      ] [16.629942, 16.118095, 17.141788]\n",
      "0 114 14257 [8.059048 0.      ] [15.121027, 16.118095, 14.123959]\n",
      "0 115 14257 [8.059048 0.      ] [14.141578, 16.118095, 12.16506]\n",
      "0 116 14257 [8.059048 0.      ] [16.361275, 16.118095, 16.604454]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-6a81d456b65e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcolorblind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-e50e80f95d95>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, sample_interval)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[1;31m# Train the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                 \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_colors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_edge\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_normalized\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                 \u001b[1;31m# Plot the progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "colorblind.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 5\n"
     ]
    }
   ],
   "source": [
    "print(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
